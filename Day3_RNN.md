# JTPA みんなでやろうDL オンライン勉強会 Day 3

Date: 5/20/2020 (Wed) 9PM

Venue: Hangout Meet (meetup参加者あてに通知されます）

Chat: [https://nocnoc.ooo/app#/chat/E3F2F42A-7FD7-4FB2-9232-B95E54C53621](https://nocnoc.ooo/app#/chat/E3F2F42A-7FD7-4FB2-9232-B95E54C53621)

<br>
  
## RNN, LSTM / NLP最新アーキテクチャ (BERT/Transfomerなど）解説


当日前半の[スライド](https://docs.google.com/presentation/d/1LCMVEfYYajOMr_W4iQUvkIvZ43f5cce5Mwvjyo5r1po/edit?usp=sharing)

数原氏による後半部分のスライドは当日参加者宛にメールにて

<br>

### 1. CNN画像処理モデルの基本をおさらい


### 2. RNN / LSTMを使ってみたい気になる

* [word2vec](https://radimrehurek.com/gensim/models/word2vec.html)
* [fastText](https://fasttext.cc/)
* [CVPR_paper_search_tool](https://github.com/jiny2001/CVPR_paper_search_tool)
* [Animated RNN, LSTM and GRU](https://towardsdatascience.com/animated-rnn-lstm-and-gru-ef124d06cf45)
* [keras LSTM Layer](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM)


## 特別ゲスト: [数原良彦](https://yoshi-suhara.com/) 氏

数原良彦 / Megagon Labs リサーチサイエンティスト

[Twitter:@suhara](https://twitter.com/suhara) / [Linkedin:](https://www.linkedin.com/in/yoshi-suhara/) / [Github:@suhara](https://github.com/suhara) / [Google Scholar:](https://scholar.google.com/citations?user=tjWt_1MAAAAJ&hl=en)

* [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)
* [Attention? Attention!](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html)
* [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
* [SYNTHESIZER: Rethinking Self-Attention in Transformer Models](https://arxiv.org/abs/2005.00743)
* [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)
* [The Stanford Question Answering Dataset](https://rajpurkar.github.io/SQuAD-explorer/)
* [GPT-2](https://openai.com/blog/better-language-models/)

補足:
* [VideoBERT: A Joint Model for Video and Language Representation Learning](https://arxiv.org/abs/1904.01766)
* [VisualBERT: A Simple and Performant Baseline for Vision and Language](https://arxiv.org/abs/1908.03557)
* [Language Models as Knowledge Bases](https://arxiv.org/abs/1909.01066)

